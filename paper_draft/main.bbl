\begin{thebibliography}{15}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[De~Cao et~al.(2021)De~Cao, Aziz, and Titov]{de2021editing}
Nicola De~Cao, Wilker Aziz, and Ivan Titov.
\newblock Editing factual knowledge in language models.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 6491--6506, 2021.

\bibitem[Elhage et~al.(2022)Elhage, Hume, Olsson, Schiefer, Henighan, Kravec,
  Hatfield-Dodds, Lasenby, Drain, Chen, et~al.]{elhage2022toy}
Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan,
  Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen,
  et~al.
\newblock Toy models of superposition.
\newblock \emph{arXiv preprint arXiv:2209.10652}, 2022.

\bibitem[French(1999)]{french1999catastrophic}
Robert~M French.
\newblock Catastrophic forgetting in connectionist networks.
\newblock \emph{Trends in Cognitive Sciences}, 3\penalty0 (4):\penalty0
  128--135, 1999.

\bibitem[Gupta et~al.(2024)Gupta, Rao, and Anand]{gupta2024model}
Akshat Gupta, Anurag Rao, and Gopala Anand.
\newblock Model editing at scale leads to gradual and catastrophic forgetting.
\newblock \emph{arXiv preprint arXiv:2401.07453}, 2024.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock In \emph{Proceedings of the National Academy of Sciences}, volume
  114, pages 3521--3526, 2017.

\bibitem[McCloskey and Cohen(1989)]{mccloskey1989catastrophic}
Michael McCloskey and Neal~J Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock \emph{Psychology of Learning and Motivation}, 24:\penalty0 109--165,
  1989.

\bibitem[Meng et~al.(2022{\natexlab{a}})Meng, Bau, Andonian, and
  Belinkov]{meng2022locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual associations in {GPT}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, pages 17359--17372, 2022{\natexlab{a}}.

\bibitem[Meng et~al.(2022{\natexlab{b}})Meng, Sharma, Andonian, Belinkov, and
  Bau]{meng2022memit}
Kevin Meng, Arnab~Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau.
\newblock Mass-editing memory in a transformer.
\newblock \emph{arXiv preprint arXiv:2210.07229}, 2022{\natexlab{b}}.

\bibitem[Nanda et~al.(2023)Nanda, Chan, Lieberum, Smith, and
  Steinhardt]{nanda2023progress}
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt.
\newblock Progress measures for grokking via mechanistic interpretability.
\newblock \emph{arXiv preprint arXiv:2301.05217}, 2023.

\bibitem[Rolnick et~al.(2019)Rolnick, Ahuja, Schwarz, Lillicrap, and
  Wayne]{rolnick2019experience}
David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
  Wayne.
\newblock Experience replay for continual learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Rusu et~al.(2016)Rusu, Rabinowitz, Desjardins, Soyer, Kirkpatrick,
  Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}
Andrei~A Rusu, Neil~C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
  Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks.
\newblock In \emph{arXiv preprint arXiv:1606.04671}, 2016.

\bibitem[Wang et~al.(2023)Wang, Zhang, Xie, Yao, Tian, Wang, Xi, Cheng, Liu,
  Zheng, et~al.]{wang2023easyedit}
Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun
  Xi, Siyuan Cheng, Kangwei Liu, Guozhou Zheng, et~al.
\newblock {EasyEdit}: An easy-to-use knowledge editing framework for large
  language models.
\newblock \emph{arXiv preprint arXiv:2308.07269}, 2023.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le,
  and Zhou]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,
  Ed~Chi, Quoc~V Le, and Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 24824--24837, 2022.

\bibitem[Yao et~al.(2023)Yao, Wang, Tian, Cheng, Li, Deng, Chen, and
  Zhang]{yao2023editing}
Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng,
  Huajun Chen, and Ningyu Zhang.
\newblock Editing large language models: Problems, methods, and opportunities.
\newblock \emph{arXiv preprint arXiv:2305.13172}, 2023.

\end{thebibliography}
