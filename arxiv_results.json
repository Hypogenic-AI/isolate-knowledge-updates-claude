[
  {
    "id": "2406.17241v4",
    "title": "Understanding Language Model Circuits through Knowledge Editing",
    "authors": [
      "Huaizhi Ge",
      "Frank Rudzicz",
      "Zining Zhu"
    ],
    "year": 2024,
    "abstract": "Recent advances in language model interpretability have identified circuits, critical subnetworks that replicate model behaviors, yet how knowledge is structured within these crucial subnetworks remains opaque. To gain an understanding toward the knowledge in the circuits, we conduct systematic knowledge editing experiments on the circuits of the GPT-2 language model. Our analysis reveals intriguing patterns in how circuits respond to editing attempts, the extent of knowledge distribution across...",
    "pdf_url": "https://arxiv.org/pdf/2406.17241v4",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2306.08302v3",
    "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
    "authors": [
      "Shirui Pan",
      "Linhao Luo",
      "Yufei Wang"
    ],
    "year": 2023,
    "abstract": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external kn...",
    "pdf_url": "https://arxiv.org/pdf/2306.08302v3",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2309.08491v1",
    "title": "Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata",
    "authors": [
      "Bohui Zhang",
      "Ioannis Reklos",
      "Nitisha Jain"
    ],
    "year": 2023,
    "abstract": "In this work, we explore the use of Large Language Models (LLMs) for knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge. For this task, given subject and relation pairs sourced from Wikidata, we utilize pre-trained LLMs to produce the relevant objects in string format and link them to their respective Wikidata QIDs. We developed a pipeline using LLMs for Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata entity mapping. The method achieved a macro-a...",
    "pdf_url": "https://arxiv.org/pdf/2309.08491v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2105.00674v1",
    "title": "Bias in Knowledge Graphs -- an Empirical Study with Movie Recommendation and Different Language Editions of DBpedia",
    "authors": [
      "Michael Matthias Voit",
      "Heiko Paulheim"
    ],
    "year": 2021,
    "abstract": "Public knowledge graphs such as DBpedia and Wikidata have been recognized as interesting sources of background knowledge to build content-based recommender systems. They can be used to add information about the items to be recommended and links between those. While quite a few approaches for exploiting knowledge graphs have been proposed, most of them aim at optimizing the recommendation strategy while using a fixed knowledge graph. In this paper, we take a different approach, i.e., we fix the r...",
    "pdf_url": "https://arxiv.org/pdf/2105.00674v1",
    "categories": [
      "cs.IR",
      "cs.AI"
    ]
  },
  {
    "id": "2503.21676v2",
    "title": "How do language models learn facts? Dynamics, curricula and hallucinations",
    "authors": [
      "Nicolas Zucchet",
      "J\u00f6rg Bornschein",
      "Stephanie Chan"
    ],
    "year": 2025,
    "abstract": "Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall....",
    "pdf_url": "https://arxiv.org/pdf/2503.21676v2",
    "categories": [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id": "2408.07413v3",
    "title": "Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge Editing for Large Language Models",
    "authors": [
      "Chenhui Hu",
      "Pengfei Cao",
      "Yubo Chen"
    ],
    "year": 2024,
    "abstract": "Knowledge editing aims to update outdated or incorrect knowledge in large language models (LLMs). However, current knowledge editing methods have limited scalability for lifelong editing. This study explores the fundamental reason why knowledge editing fails in lifelong editing. We begin with the closed-form solution derived from linear associative memory, which underpins state-of-the-art knowledge editing methods. We extend the solution from single editing to lifelong editing, and through rigor...",
    "pdf_url": "https://arxiv.org/pdf/2408.07413v3",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2502.20988v2",
    "title": "Reviewing Clinical Knowledge in Medical Large Language Models: Training and Beyond",
    "authors": [
      "Qiyuan Li",
      "Haijiang Liu",
      "Caicai Guo"
    ],
    "year": 2025,
    "abstract": "The large-scale development of large language models (LLMs) in medical contexts, such as diagnostic assistance and treatment recommendations, necessitates that these models possess accurate medical knowledge and deliver traceable decision-making processes. Clinical knowledge, encompassing the insights gained from research on the causes, prognosis, diagnosis, and treatment of diseases, has been extensively examined within real-world medical practices. Recently, there has been a notable increase i...",
    "pdf_url": "https://arxiv.org/pdf/2502.20988v2",
    "categories": [
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id": "2504.19565v3",
    "title": "Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training",
    "authors": [
      "Meng Xiao",
      "Xunxin Cai",
      "Qingqing Long"
    ],
    "year": 2025,
    "abstract": "Corpus distillation for biomedical large language models (LLMs) seeks to address the pressing challenge of insufficient quantity and quality in open-source annotated scientific corpora, which remains a bottleneck for effective LLM training in biomedical research. This paper proposes a knowledge-driven, agentic framework for scientific corpus distillation, tailored explicitly for LLM training in the biomedical domain, addressing the challenge posed by the complex hierarchy of biomedical knowledge...",
    "pdf_url": "https://arxiv.org/pdf/2504.19565v3",
    "categories": [
      "cs.CL",
      "cs.AI",
      "q-bio.QM"
    ]
  },
  {
    "id": "2406.17253v3",
    "title": "How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?",
    "authors": [
      "Huaizhi Ge",
      "Frank Rudzicz",
      "Zining Zhu"
    ],
    "year": 2024,
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but updating their knowledge post-training remains a critical challenge. While recent model editing techniques like Rank-One Model Editing (ROME) show promise, their effectiveness may vary based on the nature of the knowledge being edited. We introduce the concept of ``perplexingness'': the degree to which new knowledge conflicts with an LLM's learned conceptual hierarchies and categorical relationships. For instance, editin...",
    "pdf_url": "https://arxiv.org/pdf/2406.17253v3",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2402.13593v1",
    "title": "Knowledge Graph Enhanced Large Language Model Editing",
    "authors": [
      "Mengqi Zhang",
      "Xiaotian Ye",
      "Qiang Liu"
    ],
    "year": 2024,
    "abstract": "Large language models (LLMs) are pivotal in advancing natural language processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and outdated knowledge. Model editing emerges as a promising solution to address these challenges. However, existing editing methods struggle to track and incorporate changes in knowledge associated with edits, which limits the generalization ability of postedit LLMs in processing edited knowledge. To tackle these problems, we propose a novel model editing...",
    "pdf_url": "https://arxiv.org/pdf/2402.13593v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2312.11539v3",
    "title": "KGLens: Towards Efficient and Effective Knowledge Probing of Large Language Models with Knowledge Graphs",
    "authors": [
      "Shangshang Zheng",
      "He Bai",
      "Yizhe Zhang"
    ],
    "year": 2023,
    "abstract": "Large Language Models (LLMs) might hallucinate facts, while curated Knowledge Graph (KGs) are typically factually reliable especially with domain-specific knowledge. Measuring the alignment between KGs and LLMs can effectively probe the factualness and identify the knowledge blind spots of LLMs. However, verifying the LLMs over extensive KGs can be expensive. In this paper, we present KGLens, a Thompson-sampling-inspired framework aimed at effectively and efficiently measuring the alignment betw...",
    "pdf_url": "https://arxiv.org/pdf/2312.11539v3",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id": "2208.11057v3",
    "title": "Prompting as Probing: Using Language Models for Knowledge Base Construction",
    "authors": [
      "Dimitrios Alivanistos",
      "Selene B\u00e1ez Santamar\u00eda",
      "Michael Cochez"
    ],
    "year": 2022,
    "abstract": "Language Models (LMs) have proven to be useful in various downstream applications, such as summarisation, translation, question answering and text classification. LMs are becoming increasingly important tools in Artificial Intelligence, because of the vast quantity of information they can store. In this work, we present ProP (Prompting as Probing), which utilizes GPT-3, a large Language Model originally proposed by OpenAI in 2020, to perform the task of Knowledge Base Construction (KBC). ProP im...",
    "pdf_url": "https://arxiv.org/pdf/2208.11057v3",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2309.08952v2",
    "title": "Cross-Lingual Knowledge Editing in Large Language Models",
    "authors": [
      "Jiaan Wang",
      "Yunlong Liang",
      "Zengkui Sun"
    ],
    "year": 2023,
    "abstract": "Knowledge editing aims to change language models' performance on several special cases (i.e., editing scope) by infusing the corresponding expected knowledge into them. With the recent advancements in large language models (LLMs), knowledge editing has been shown as a promising technique to adapt LLMs to new knowledge without retraining from scratch. However, most of the previous studies neglect the multi-lingual nature of some main-stream LLMs (e.g., LLaMA, ChatGPT and GPT-4), and typically foc...",
    "pdf_url": "https://arxiv.org/pdf/2309.08952v2",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2404.17000v1",
    "title": "Evaluating Class Membership Relations in Knowledge Graphs using Large Language Models",
    "authors": [
      "Bradley P. Allen",
      "Paul T. Groth"
    ],
    "year": 2024,
    "abstract": "A backbone of knowledge graphs are their class membership relations, which assign entities to a given class. As part of the knowledge engineering process, we propose a new method for evaluating the quality of these relations by processing descriptions of a given entity and class using a zero-shot chain-of-thought classifier that uses a natural language intensional definition of a class. We evaluate the method using two publicly available knowledge graphs, Wikidata and CaLiGraph, and 7 large lang...",
    "pdf_url": "https://arxiv.org/pdf/2404.17000v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2312.05434v1",
    "title": "Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models",
    "authors": [
      "Hongzhan Lin",
      "Ziyang Luo",
      "Jing Ma"
    ],
    "year": 2023,
    "abstract": "The age of social media is rife with memes. Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image. However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image. In this paper, we attempt to detect harmful memes based on advanced reasoning over the ...",
    "pdf_url": "https://arxiv.org/pdf/2312.05434v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2206.06520v1",
    "title": "Memory-Based Model Editing at Scale",
    "authors": [
      "Eric Mitchell",
      "Charles Lin",
      "Antoine Bosselut"
    ],
    "year": 2022,
    "abstract": "Even the largest neural networks make errors, and once-correct predictions can become invalid as the world changes. Model editors make local updates to the behavior of base (pre-trained) models to inject updated knowledge or correct undesirable behaviors. Existing model editors have shown promise, but also suffer from insufficient expressiveness: they struggle to accurately model an edit's intended scope (examples affected by the edit), leading to inaccurate predictions for test inputs loosely r...",
    "pdf_url": "https://arxiv.org/pdf/2206.06520v1",
    "categories": [
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id": "2409.20500v1",
    "title": "FreeMask: Rethinking the Importance of Attention Masks for Zero-Shot Video Editing",
    "authors": [
      "Lingling Cai",
      "Kang Zhao",
      "Hangjie Yuan"
    ],
    "year": 2024,
    "abstract": "Text-to-video diffusion models have made remarkable advancements. Driven by their ability to generate temporally coherent videos, research on zero-shot video editing using these fundamental models has expanded rapidly. To enhance editing quality, structural controls are frequently employed in video editing. Among these techniques, cross-attention mask control stands out for its effectiveness and efficiency. However, when cross-attention masks are naively applied to video editing, they can introd...",
    "pdf_url": "https://arxiv.org/pdf/2409.20500v1",
    "categories": [
      "cs.CV",
      "cs.MM"
    ]
  },
  {
    "id": "2110.11309v2",
    "title": "Fast Model Editing at Scale",
    "authors": [
      "Eric Mitchell",
      "Charles Lin",
      "Antoine Bosselut"
    ],
    "year": 2021,
    "abstract": "While large pre-trained models have enabled impressive results on a variety of downstream tasks, the largest existing models still make errors, and even accurate predictions may become outdated over time. Because detecting all such failures at training time is impossible, enabling both developers and end users of such models to correct inaccurate outputs while leaving the model otherwise intact is desirable. However, the distributed, black-box nature of the representations learned by large neura...",
    "pdf_url": "https://arxiv.org/pdf/2110.11309v2",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id": "2409.16535v1",
    "title": "Prompt Sliders for Fine-Grained Control, Editing and Erasing of Concepts in Diffusion Models",
    "authors": [
      "Deepak Sridhar",
      "Nuno Vasconcelos"
    ],
    "year": 2024,
    "abstract": "Diffusion models have recently surpassed GANs in image synthesis and editing, offering superior image quality and diversity. However, achieving precise control over attributes in generated images remains a challenge. Concept Sliders introduced a method for fine-grained image control and editing by learning concepts (attributes/objects). However, this approach adds parameters and increases inference time due to the loading and unloading of Low-Rank Adapters (LoRAs) used for learning concepts. The...",
    "pdf_url": "https://arxiv.org/pdf/2409.16535v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2406.14555v1",
    "title": "A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models",
    "authors": [
      "Xincheng Shuai",
      "Henghui Ding",
      "Xingjun Ma"
    ],
    "year": 2024,
    "abstract": "Image editing aims to edit the given synthetic or real image to meet the specific requirements from users. It is widely studied in recent years as a promising and challenging field of Artificial Intelligence Generative Content (AIGC). Recent significant advancement in this field is based on the development of text-to-image (T2I) diffusion models, which generate images according to text prompts. These models demonstrate remarkable generative capabilities and have become widely used tools for imag...",
    "pdf_url": "https://arxiv.org/pdf/2406.14555v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2303.17599v3",
    "title": "Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models",
    "authors": [
      "Wen Wang",
      "Yan Jiang",
      "Kangyang Xie"
    ],
    "year": 2023,
    "abstract": "Large-scale text-to-image diffusion models achieve unprecedented success in image generation and editing. However, how to extend such success to video editing is unclear. Recent initial attempts at video editing require significant text-to-video data and computation resources for training, which is often not accessible. In this work, we propose vid2vid-zero, a simple yet effective method for zero-shot video editing. Our vid2vid-zero leverages off-the-shelf image diffusion models, and doesn't req...",
    "pdf_url": "https://arxiv.org/pdf/2303.17599v3",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2601.01915v1",
    "title": "TalkPhoto: A Versatile Training-Free Conversational Assistant for Intelligent Image Editing",
    "authors": [
      "Yujie Hu",
      "Zecheng Tang",
      "Xu Jiang"
    ],
    "year": 2026,
    "abstract": "Thanks to the powerful language comprehension capabilities of Large Language Models (LLMs), existing instruction-based image editing methods have introduced Multimodal Large Language Models (MLLMs) to promote information exchange between instructions and images, ensuring the controllability and flexibility of image editing. However, these frameworks often build a multi-instruction dataset to train the model to handle multiple editing tasks, which is not only time-consuming and labor-intensive bu...",
    "pdf_url": "https://arxiv.org/pdf/2601.01915v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2512.00677v1",
    "title": "Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer",
    "authors": [
      "Dong In Lee",
      "Hyungjun Doh",
      "Seunggeun Chi"
    ],
    "year": 2025,
    "abstract": "Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction. However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing. Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing. We introduce Dynamic-eDiTor, a traini...",
    "pdf_url": "https://arxiv.org/pdf/2512.00677v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id": "2506.20967v2",
    "title": "DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing",
    "authors": [
      "Lingling Cai",
      "Kang Zhao",
      "Hangjie Yuan"
    ],
    "year": 2025,
    "abstract": "The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in video generation. However, directly applying existing video editing methods to Video DiTs often incurs substantial computational overhead, due to resource-intensive attention modification or finetuning. To alleviate this problem, we present DFVEdit, an efficient zero-shot video editing method tailored for Video DiTs. DFVEdit eliminates the need for both attention modification and fine-tuning by directly operating on cle...",
    "pdf_url": "https://arxiv.org/pdf/2506.20967v2",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id": "2404.04526v2",
    "title": "DATENeRF: Depth-Aware Text-based Editing of NeRFs",
    "authors": [
      "Sara Rojas",
      "Julien Philip",
      "Kai Zhang"
    ],
    "year": 2024,
    "abstract": "Recent advancements in diffusion models have shown remarkable proficiency in editing 2D images based on text prompts. However, extending these techniques to edit scenes in Neural Radiance Fields (NeRF) is complex, as editing individual 2D frames can result in inconsistencies across multiple views. Our crucial insight is that a NeRF scene's geometry can serve as a bridge to integrate these 2D edits. Utilizing this geometry, we employ a depth-conditioned ControlNet to enhance the coherence of each...",
    "pdf_url": "https://arxiv.org/pdf/2404.04526v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2503.05212v1",
    "title": "Knowledge Updating? No More Model Editing! Just Selective Contextual Reasoning",
    "authors": [
      "Guoxiu He",
      "Xin Song",
      "Aixin Sun"
    ],
    "year": 2025,
    "abstract": "As real-world knowledge evolves, the information embedded within large language models (LLMs) can become outdated, inadequate, or erroneous. Model editing has emerged as a prominent approach for updating LLMs' knowledge with minimal computational costs and parameter changes. This approach typically identifies and adjusts specific model parameters associated with newly acquired knowledge. However, existing methods often underestimate the adverse effects that parameter modifications can have on br...",
    "pdf_url": "https://arxiv.org/pdf/2503.05212v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2506.13638v2",
    "title": "DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models",
    "authors": [
      "Zhiyi Shi",
      "Binjie Wang",
      "Chongjie Si"
    ],
    "year": 2025,
    "abstract": "Model editing aims to efficiently update a pre-trained model's knowledge without the need for time-consuming full retraining. While existing pioneering editing methods achieve promising results, they primarily focus on editing single-modal language models (LLMs). However, for vision-language models (VLMs), which involve multiple modalities, the role and impact of each modality on editing performance remain largely unexplored. To address this gap, we explore the impact of textual and visual modal...",
    "pdf_url": "https://arxiv.org/pdf/2506.13638v2",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id": "2308.00135v3",
    "title": "InFusion: Inject and Attention Fusion for Multi Concept Zero-Shot Text-based Video Editing",
    "authors": [
      "Anant Khandelwal"
    ],
    "year": 2023,
    "abstract": "Large text-to-image diffusion models have achieved remarkable success in generating diverse, high-quality images. Additionally, these models have been successfully leveraged to edit input images by just changing the text prompt. But when these models are applied to videos, the main challenge is to ensure temporal consistency and coherence across frames. In this paper, we propose InFusion, a framework for zero-shot text-based video editing leveraging large pre-trained image diffusion models. Our ...",
    "pdf_url": "https://arxiv.org/pdf/2308.00135v3",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2201.13433v1",
    "title": "Third Time's the Charm? Image and Video Editing with StyleGAN3",
    "authors": [
      "Yuval Alaluf",
      "Or Patashnik",
      "Zongze Wu"
    ],
    "year": 2022,
    "abstract": "StyleGAN is arguably one of the most intriguing and well-studied generative models, demonstrating impressive performance in image generation, inversion, and manipulation. In this work, we explore the recent StyleGAN3 architecture, compare it to its predecessor, and investigate its unique advantages, as well as drawbacks. In particular, we demonstrate that while StyleGAN3 can be trained on unaligned data, one can still use aligned data for training, without hindering the ability to generate unali...",
    "pdf_url": "https://arxiv.org/pdf/2201.13433v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2512.10284v2",
    "title": "MotionEdit: Benchmarking and Learning Motion-Centric Image Editing",
    "authors": [
      "Yixin Wan",
      "Lei Ke",
      "Wenhao Yu"
    ],
    "year": 2025,
    "abstract": "We introduce MotionEdit, a novel dataset for motion-centric image editing-the task of modifying subject actions and interactions while preserving identity, structure, and physical plausibility. Unlike existing image editing datasets that focus on static appearance changes or contain only sparse, low-quality motion edits, MotionEdit provides high-fidelity image pairs depicting realistic motion transformations extracted and verified from continuous videos. This new task is not only scientifically ...",
    "pdf_url": "https://arxiv.org/pdf/2512.10284v2",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id": "9503001v1",
    "title": "Stellarator News, Issue 38, March 1995",
    "authors": [
      "James A. Rome"
    ],
    "year": 1995,
    "abstract": "Stellarator News, an international journal of the stellarator community, is Published by Fusion Energy Division, Oak Ridge National Laboratory, James A. Rome, Editor\n  In the March 1995 issue . . .\n  **** Exerpts from the U.S. Congress Office of Technology Assment report on TPX and Alternate Concepts.\n  **** Edge transport and turbulence studies on U-3M\n  The turbulent-driven particle flow is shown to be comparable with the equilibrium flow at the boundary of the configuration under Alfven-heati...",
    "pdf_url": "https://arxiv.org/pdf/plasm-ph/9503001v1",
    "categories": [
      "physics.plasm-ph"
    ]
  },
  {
    "id": "2502.07322v3",
    "title": "MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject Batch Editing for LLMs",
    "authors": [
      "Zilu Dong",
      "Xiangqing Shen",
      "Rui Xia"
    ],
    "year": 2025,
    "abstract": "As large language models continue to scale up, knowledge editing techniques that modify models' internal knowledge without full retraining have gained significant attention. MEMIT, a prominent batch editing algorithm, stands out for its capability to perform mass knowledge modifications. However, we uncover that MEMIT's editing efficacy significantly deteriorates when processing batches containing multiple edits sharing the same subject. Our analysis reveals this stems from MEMIT's key value mod...",
    "pdf_url": "https://arxiv.org/pdf/2502.07322v3",
    "categories": [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id": "2403.07175v3",
    "title": "Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing",
    "authors": [
      "Akshat Gupta",
      "Sidharth Baskaran",
      "Gopala Anumanchipalli"
    ],
    "year": 2024,
    "abstract": "Recent work using Rank-One Model Editing (ROME), a popular model editing method, has shown that there are certain facts that the algorithm is unable to edit without breaking the model. Such edits have previously been called disabling edits. These disabling edits cause immediate model collapse and limits the use of ROME for sequential editing. In this paper, we show that disabling edits are an artifact of irregularities in the implementation of ROME. With this paper, we provide a more stable impl...",
    "pdf_url": "https://arxiv.org/pdf/2403.07175v3",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "1307.6116v2",
    "title": "FACT - Long-term stability and observations during strong Moon light",
    "authors": [
      "M. L. Knoetig",
      "A. Biland",
      "T. Bretz"
    ],
    "year": 2013,
    "abstract": "The First G-APD Cherenkov Telescope (FACT) is the first Cherenkov telescope equipped with a camera made of silicon photon detectors (G-APD aka. SiPM). Since October 2011, it is regularly taking data on the Canary Island of La Palma. G-APDs are ideal detectors for Cherenkov telescopes as they are robust and stable. Furthermore, the insensitivity of G-APDs towards strong ambient light allows to conduct observations during bright Moon and twilight. This gain in observation time is essential for the...",
    "pdf_url": "https://arxiv.org/pdf/1307.6116v2",
    "categories": [
      "astro-ph.IM",
      "astro-ph.HE"
    ]
  },
  {
    "id": "9909026v1",
    "title": "Highlights of the Rome Workshop on Gamma-Ray Bursts in the Afterglow Era",
    "authors": [
      "D. Q. Lamb"
    ],
    "year": 1999,
    "abstract": "I review some of the highlights of the Rome Workshop on Gamma-Ray Bursts, and discuss some of the questions these results pose about the nature and origin of gamma-ray bursts.",
    "pdf_url": "https://arxiv.org/pdf/astro-ph/9909026v1",
    "categories": [
      "astro-ph"
    ]
  },
  {
    "id": "2403.14236v5",
    "title": "A Unified Framework for Model Editing",
    "authors": [
      "Akshat Gupta",
      "Dev Sajnani",
      "Gopala Anumanchipalli"
    ],
    "year": 2024,
    "abstract": "ROME and MEMIT are largely believed to be two different model editing algorithms, with the major difference between them being the ability to perform batched edits. In this paper, we unify these two algorithms under a single conceptual umbrella, optimizing for the same goal, which we call the preservation-memorization objective. ROME uses an equality constraint to optimize this objective to perform one edit at a time, whereas MEMIT employs a more flexible least-square constraint that allows for ...",
    "pdf_url": "https://arxiv.org/pdf/2403.14236v5",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id": "2401.07453v4",
    "title": "Model Editing at Scale leads to Gradual and Catastrophic Forgetting",
    "authors": [
      "Akshat Gupta",
      "Anurag Rao",
      "Gopala Anumanchipalli"
    ],
    "year": 2024,
    "abstract": "Editing knowledge in large language models is an attractive capability to have which allows us to correct incorrectly learnt facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the sam...",
    "pdf_url": "https://arxiv.org/pdf/2401.07453v4",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ]
  },
  {
    "id": "1609.04846v1",
    "title": "A Tutorial about Random Neural Networks in Supervised Learning",
    "authors": [
      "Sebasti\u00e1n Basterrech",
      "Gerardo Rubino"
    ],
    "year": 2016,
    "abstract": "Random Neural Networks (RNNs) are a class of Neural Networks (NNs) that can also be seen as a specific type of queuing network. They have been successfully used in several domains during the last 25 years, as queuing networks to analyze the performance of resource sharing in many engineering areas, as learning tools and in combinatorial optimization, where they are seen as neural systems, and also as models of neurological aspects of living beings. In this article we focus on their learning capa...",
    "pdf_url": "https://arxiv.org/pdf/1609.04846v1",
    "categories": [
      "cs.NE"
    ]
  },
  {
    "id": "2502.01654v1",
    "title": "Predicting concentration levels of air pollutants by transfer learning and recurrent neural network",
    "authors": [
      "Iat Hang Fong",
      "Tengyue Li",
      "Simon Fong"
    ],
    "year": 2025,
    "abstract": "Air pollution (AP) poses a great threat to human health, and people are paying more attention than ever to its prediction. Accurate prediction of AP helps people to plan for their outdoor activities and aids protecting human health. In this paper, long-short term memory (LSTM) recurrent neural networks (RNNs) have been used to predict the future concentration of air pollutants (APS) in Macau. Additionally, meteorological data and data on the concentration of APS have been utilized. Moreover, in ...",
    "pdf_url": "https://arxiv.org/pdf/2502.01654v1",
    "categories": [
      "cs.LG",
      "cs.NE",
      "physics.ao-ph"
    ]
  },
  {
    "id": "1803.02421v2",
    "title": "Masked Conditional Neural Networks for Audio Classification",
    "authors": [
      "Fady Medhat",
      "David Chesmore",
      "John Robinson"
    ],
    "year": 2018,
    "abstract": "We present the ConditionaL Neural Network (CLNN) and the Masked ConditionaL Neural Network (MCLNN) designed for temporal signal recognition. The CLNN takes into consideration the temporal nature of the sound signal and the MCLNN extends upon the CLNN through a binary mask to preserve the spatial locality of the features and allows an automated exploration of the features combination analogous to hand-crafting the most relevant features for the recognition task. MCLNN has achieved competitive rec...",
    "pdf_url": "https://arxiv.org/pdf/1803.02421v2",
    "categories": [
      "stat.ML",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ]
  },
  {
    "id": "2009.03192v2",
    "title": "A Neural Network Perturbation Theory Based on the Born Series",
    "authors": [
      "Bastian Kaspschak",
      "Ulf-G. Mei\u00dfner"
    ],
    "year": 2020,
    "abstract": "Deep Learning using the eponymous deep neural networks (DNNs) has become an attractive approach towards various data-based problems of theoretical physics in the past decade. There has been a clear trend to deeper architectures containing increasingly more powerful and involved layers. Contrarily, Taylor coefficients of DNNs still appear mainly in the light of interpretability studies, where they are computed at most to first order. However, especially in theoretical physics numerous problems be...",
    "pdf_url": "https://arxiv.org/pdf/2009.03192v2",
    "categories": [
      "cs.LG",
      "nucl-th",
      "physics.comp-ph",
      "stat.ML"
    ]
  },
  {
    "id": "2306.14753v1",
    "title": "The Deep Arbitrary Polynomial Chaos Neural Network or how Deep Artificial Neural Networks could benefit from Data-Driven Homogeneous Chaos Theory",
    "authors": [
      "Sergey Oladyshkin",
      "Timothy Praditia",
      "Ilja Kr\u00f6ker"
    ],
    "year": 2023,
    "abstract": "Artificial Intelligence and Machine learning have been widely used in various fields of mathematical computing, physical modeling, computational science, communication science, and stochastic analysis. Approaches based on Deep Artificial Neural Networks (DANN) are very popular in our days. Depending on the learning task, the exact form of DANNs is determined via their multi-layer architecture, activation functions and the so-called loss function. However, for a majority of deep learning approach...",
    "pdf_url": "https://arxiv.org/pdf/2306.14753v1",
    "categories": [
      "cs.NE",
      "stat.ML"
    ]
  },
  {
    "id": "1908.02146v2",
    "title": "Knowledge Query Network: How Knowledge Interacts with Skills",
    "authors": [
      "Jinseok Lee",
      "Dit-Yan Yeung"
    ],
    "year": 2019,
    "abstract": "Knowledge Tracing (KT) is to trace the knowledge of students as they solve a sequence of problems represented by their related skills. This involves abstract concepts of students' states of knowledge and the interactions between those states and skills. Therefore, a KT model is designed to predict whether students will give correct answers and to describe such abstract concepts. However, existing methods either give relatively low prediction accuracy or fail to explain those concepts intuitively...",
    "pdf_url": "https://arxiv.org/pdf/1908.02146v2",
    "categories": [
      "cs.CY",
      "cs.LG"
    ]
  },
  {
    "id": "1807.02477v1",
    "title": "Development of a sensory-neural network for medical diagnosing",
    "authors": [
      "Igor Grabec",
      "Eva \u0160vegl",
      "Mihael Sok"
    ],
    "year": 2018,
    "abstract": "Performance of a sensory-neural network developed for diagnosing of diseases is described. Information about patient's condition is provided by answers to the questionnaire. Questions correspond to sensors generating signals when patients acknowledge symptoms. These signals excite neurons in which characteristics of the diseases are represented by synaptic weights associated with indicators of symptoms. The disease corresponding to the most excited neuron is proposed as the result of diagnosing....",
    "pdf_url": "https://arxiv.org/pdf/1807.02477v1",
    "categories": [
      "cs.NE"
    ]
  },
  {
    "id": "1901.06610v2",
    "title": "Hierarchical Attentional Hybrid Neural Networks for Document Classification",
    "authors": [
      "Jader Abreu",
      "Luis Fred",
      "David Mac\u00eado"
    ],
    "year": 2019,
    "abstract": "Document classification is a challenging task with important applications. The deep learning approaches to the problem have gained much attention recently. Despite the progress, the proposed models do not incorporate the knowledge of the document structure in the architecture efficiently and not take into account the contexting importance of words and sentences. In this paper, we propose a new approach based on a combination of convolutional neural networks, gated recurrent units, and attention ...",
    "pdf_url": "https://arxiv.org/pdf/1901.06610v2",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ]
  },
  {
    "id": "1906.10015v2",
    "title": "A Review on Neural Network Models of Schizophrenia and Autism Spectrum Disorder",
    "authors": [
      "Pablo Lanillos",
      "Daniel Oliva",
      "Anja Philippsen"
    ],
    "year": 2019,
    "abstract": "This survey presents the most relevant neural network models of autism spectrum disorder and schizophrenia, from the first connectionist models to recent deep network architectures. We analyzed and compared the most representative symptoms with its neural model counterpart, detailing the alteration introduced in the network that generates each of the symptoms, and identifying their strengths and weaknesses. We additionally cross-compared Bayesian and free-energy approaches, as they are widely ap...",
    "pdf_url": "https://arxiv.org/pdf/1906.10015v2",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.NE"
    ]
  },
  {
    "id": "2212.06370v4",
    "title": "Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation",
    "authors": [
      "Giorgio Morales",
      "John W. Sheppard"
    ],
    "year": 2022,
    "abstract": "Accurate uncertainty quantification is necessary to enhance the reliability of deep learning models in real-world applications. In the case of regression tasks, prediction intervals (PIs) should be provided along with the deterministic predictions of deep learning models. Such PIs are useful or \"high-quality\" as long as they are sufficiently narrow and capture most of the probability density. In this paper, we present a method to learn prediction intervals for regression-based neural networks au...",
    "pdf_url": "https://arxiv.org/pdf/2212.06370v4",
    "categories": [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id": "1411.1792v1",
    "title": "How transferable are features in deep neural networks?",
    "authors": [
      "Jason Yosinski",
      "Jeff Clune",
      "Yoshua Bengio"
    ],
    "year": 2014,
    "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experime...",
    "pdf_url": "https://arxiv.org/pdf/1411.1792v1",
    "categories": [
      "cs.LG",
      "cs.NE"
    ]
  },
  {
    "id": "2304.09590v1",
    "title": "Parallel Neural Networks in Golang",
    "authors": [
      "Daniela Kalwarowskyj",
      "Erich Schikuta"
    ],
    "year": 2023,
    "abstract": "This paper describes the design and implementation of parallel neural networks (PNNs) with the novel programming language Golang. We follow in our approach the classical Single-Program Multiple-Data (SPMD) model where a PNN is composed of several sequential neural networks, which are trained with a proportional share of the training dataset. We used for this purpose the MNIST dataset, which contains binary images of handwritten digits. Our analysis focusses on different activation functions and ...",
    "pdf_url": "https://arxiv.org/pdf/2304.09590v1",
    "categories": [
      "cs.NE",
      "cs.DC"
    ]
  },
  {
    "id": "2207.14782v1",
    "title": "Minimal Neural Atlas: Parameterizing Complex Surfaces with Minimal Charts and Distortion",
    "authors": [
      "Weng Fei Low",
      "Gim Hee Lee"
    ],
    "year": 2022,
    "abstract": "Explicit neural surface representations allow for exact and efficient extraction of the encoded surface at arbitrary precision, as well as analytic derivation of differential geometric properties such as surface normal and curvature. Such desirable properties, which are absent in its implicit counterpart, makes it ideal for various applications in computer vision, graphics and robotics. However, SOTA works are limited in terms of the topology it can effectively describe, distortion it introduces...",
    "pdf_url": "https://arxiv.org/pdf/2207.14782v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ]
  },
  {
    "id": "1606.00825v2",
    "title": "Training a Hidden Markov Model with a Bayesian Spiking Neural Network",
    "authors": [
      "Amirhossein Tavanaei",
      "Anthony S Maida"
    ],
    "year": 2016,
    "abstract": "It is of some interest to understand how statistically based mechanisms for signal processing might be integrated with biologically motivated mechanisms such as neural networks. This paper explores a novel hybrid approach for classifying segments of sequential data, such as individual spoken works. The approach combines a hidden Markov model (HMM) with a spiking neural network (SNN). The HMM, consisting of states and transitions, forms a fixed backbone with nonadaptive transition probabilities. ...",
    "pdf_url": "https://arxiv.org/pdf/1606.00825v2",
    "categories": [
      "cs.NE"
    ]
  },
  {
    "id": "1802.02271v2",
    "title": "Universal Deep Neural Network Compression",
    "authors": [
      "Yoojin Choi",
      "Mostafa El-Khamy",
      "Jungwon Lee"
    ],
    "year": 2018,
    "abstract": "In this paper, we investigate lossy compression of deep neural networks (DNNs) by weight quantization and lossless source coding for memory-efficient deployment. Whereas the previous work addressed non-universal scalar quantization and entropy coding of DNN weights, we for the first time introduce universal DNN compression by universal vector quantization and universal source coding. In particular, we examine universal randomized lattice quantization of DNNs, which randomizes DNN weights by unif...",
    "pdf_url": "https://arxiv.org/pdf/1802.02271v2",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.NE"
    ]
  },
  {
    "id": "2202.05262v5",
    "title": "Locating and Editing Factual Associations in GPT",
    "authors": [
      "Kevin Meng",
      "David Bau",
      "Alex Andonian"
    ],
    "year": 2022,
    "abstract": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that the...",
    "pdf_url": "https://arxiv.org/pdf/2202.05262v5",
    "categories": [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id": "2404.03646v2",
    "title": "Locating and Editing Factual Associations in Mamba",
    "authors": [
      "Arnab Sen Sharma",
      "David Atkinson",
      "David Bau"
    ],
    "year": 2024,
    "abstract": "We investigate the mechanisms of factual recall in the Mamba state space model. Our work is inspired by previous findings in autoregressive transformer language models suggesting that their knowledge recall is localized to particular modules at specific token locations; we therefore ask whether factual recall in Mamba can be similarly localized. To investigate this, we conduct four lines of experiments on Mamba. First, we apply causal tracing or interchange interventions to localize key componen...",
    "pdf_url": "https://arxiv.org/pdf/2404.03646v2",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2305.05516v2",
    "title": "GPT in Game Theory Experiments",
    "authors": [
      "Fulin Guo"
    ],
    "year": 2023,
    "abstract": "This paper explores the use of Generative Pre-trained Transformers (GPT) in strategic game experiments, specifically the ultimatum game and the prisoner's dilemma. I designed prompts and architectures to enable GPT to understand the game rules and to generate both its choices and the reasoning behind decisions. The key findings show that GPT exhibits behaviours similar to human responses, such as making positive offers and rejecting unfair ones in the ultimatum game, along with conditional coope...",
    "pdf_url": "https://arxiv.org/pdf/2305.05516v2",
    "categories": [
      "econ.GN"
    ]
  },
  {
    "id": "2410.06331v3",
    "title": "Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing",
    "authors": [
      "Zhuoran Zhang",
      "Yongxiang Li",
      "Zijian Kan"
    ],
    "year": 2024,
    "abstract": "The locate-then-edit paradigm has shown significant promise for knowledge editing (KE) in Large Language Models (LLMs). While previous methods perform well on single-hop fact recall tasks, they consistently struggle with multi-hop factual recall tasks involving newly edited knowledge. In this paper, leveraging tools in mechanistic interpretability, we first identify that in multi-hop tasks, LLMs tend to retrieve knowledge with implicit subject information from deeper MLP layers, unlike single-ho...",
    "pdf_url": "https://arxiv.org/pdf/2410.06331v3",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id": "2408.15091v3",
    "title": "Relation Also Knows: Rethinking the Recall and Editing of Factual Associations in Auto-Regressive Transformer Language Models",
    "authors": [
      "Xiyu Liu",
      "Zhengxiao Liu",
      "Naibin Gu"
    ],
    "year": 2024,
    "abstract": "The storage and recall of factual associations in auto-regressive transformer language models (LMs) have drawn a great deal of attention, inspiring knowledge editing by directly modifying the located model weights. Most editing works achieve knowledge editing under the guidance of existing interpretations of knowledge recall that mainly focus on subject knowledge. However, these interpretations are seriously flawed, neglecting relation information and leading to the over-generalizing problem for...",
    "pdf_url": "https://arxiv.org/pdf/2408.15091v3",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2406.16416v2",
    "title": "Multilingual Knowledge Editing with Language-Agnostic Factual Neurons",
    "authors": [
      "Xue Zhang",
      "Yunlong Liang",
      "Fandong Meng"
    ],
    "year": 2024,
    "abstract": "Multilingual knowledge editing (MKE) aims to simultaneously update factual knowledge across multiple languages within large language models (LLMs). Previous research indicates that the same knowledge across different languages within LLMs exhibits a degree of shareability. However, most existing MKE methods overlook the connections of the same knowledge between different languages, resulting in knowledge conflicts and limited edit performance. To address this issue, we first investigate how LLMs...",
    "pdf_url": "https://arxiv.org/pdf/2406.16416v2",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2402.13919v4",
    "title": "SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization",
    "authors": [
      "Prakamya Mishra",
      "Zonghai Yao",
      "Parth Vashisht"
    ],
    "year": 2024,
    "abstract": "Large Language Models (LLMs) such as GPT & Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes >100B parameter GPT variants like GPT-3.5 & GPT-4 to act as synthetic experts to generate hi...",
    "pdf_url": "https://arxiv.org/pdf/2402.13919v4",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2601.01957v1",
    "title": "AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing",
    "authors": [
      "Tianbo Wang",
      "Yuqing Ma",
      "Kewei Liao"
    ],
    "year": 2026,
    "abstract": "Large Vision-Language Models (LVLMs) have achieved substantial progress in cross-modal tasks. However, due to language bias, LVLMs are susceptible to object hallucination, which can be primarily divided into category, attribute, and relation hallucination, significantly impeding the trustworthy AI applications. Editing the internal activations of LVLMs has shown promising effectiveness in mitigating hallucinations with minimal cost. However, previous editing approaches neglect the effective guid...",
    "pdf_url": "https://arxiv.org/pdf/2601.01957v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2310.20033v2",
    "title": "Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization",
    "authors": [
      "Prakamya Mishra",
      "Zonghai Yao",
      "Shuwei Chen"
    ],
    "year": 2023,
    "abstract": "Large Language Models (LLMs) like the GPT and LLaMA families have demonstrated exceptional capabilities in capturing and condensing critical contextual information and achieving state-of-the-art performance in the summarization task. However, community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually inco...",
    "pdf_url": "https://arxiv.org/pdf/2310.20033v2",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2410.10859v2",
    "title": "FAME: Towards Factual Multi-Task Model Editing",
    "authors": [
      "Li Zeng",
      "Yingyu Shan",
      "Zeming Liu"
    ],
    "year": 2024,
    "abstract": "Large language models (LLMs) embed extensive knowledge and utilize it to perform exceptionally well across various tasks. Nevertheless, outdated knowledge or factual errors within LLMs can lead to misleading or incorrect responses, causing significant issues in practical applications. To rectify the fatal flaw without the necessity for costly model retraining, various model editing approaches have been proposed to correct inaccurate knowledge within LLMs in a cost-efficient way. To evaluate thes...",
    "pdf_url": "https://arxiv.org/pdf/2410.10859v2",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2502.02173v1",
    "title": "Mass-Editing Memory with Attention in Transformers: A cross-lingual exploration of knowledge",
    "authors": [
      "Daniel Tamayo",
      "Aitor Gonzalez-Agirre",
      "Javier Hernando"
    ],
    "year": 2025,
    "abstract": "Recent research has explored methods for updating and modifying factual knowledge in large language models, often focusing on specific multi-layer perceptron blocks. This study expands on this work by examining the effectiveness of existing knowledge editing methods across languages and delving into the role of attention mechanisms in this process. Drawing from the insights gained, we propose Mass-Editing Memory with Attention in Transformers (MEMAT), a method that achieves significant improveme...",
    "pdf_url": "https://arxiv.org/pdf/2502.02173v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "1705.01015v3",
    "title": "Deep Learning for Tumor Classification in Imaging Mass Spectrometry",
    "authors": [
      "Jens Behrmann",
      "Christian Etmann",
      "Tobias Boskamp"
    ],
    "year": 2017,
    "abstract": "Motivation: Tumor classification using Imaging Mass Spectrometry (IMS) data has a high potential for future applications in pathology. Due to the complexity and size of the data, automated feature extraction and classification steps are required to fully process the data. Deep learning offers an approach to learn feature extraction and classification combined in a single model. Commonly these steps are handled separately in IMS data analysis, hence deep learning offers an alternative strategy wo...",
    "pdf_url": "https://arxiv.org/pdf/1705.01015v3",
    "categories": [
      "stat.ML",
      "cs.LG"
    ]
  },
  {
    "id": "1303.1343v1",
    "title": "Nuclear Masses and Neutron Stars",
    "authors": [
      "Susanne Kreim",
      "Matthias Hempel",
      "David Lunney"
    ],
    "year": 2013,
    "abstract": "Precision mass spectrometry of neutron-rich nuclei is of great relevance for astrophysics. Masses of exotic nuclides impose constraints on models for the nuclear interaction and thus affect the description of the equation of state of nuclear matter, which can be extended to describe neutron-star matter. With knowledge of the masses of nuclides near shell closures, one can also derive the neutron-star crustal composition. The Penning-trap mass spectrometer ISOLTRAP at CERN-ISOLDE has recently ach...",
    "pdf_url": "https://arxiv.org/pdf/1303.1343v1",
    "categories": [
      "nucl-th",
      "astro-ph.SR"
    ]
  },
  {
    "id": "2210.08737v1",
    "title": "Temporal and Contextual Transformer for Multi-Camera Editing of TV Shows",
    "authors": [
      "Anyi Rao",
      "Xuekun Jiang",
      "Sichen Wang"
    ],
    "year": 2022,
    "abstract": "The ability to choose an appropriate camera view among multiple cameras plays a vital role in TV shows delivery. But it is hard to figure out the statistical pattern and apply intelligent processing due to the lack of high-quality training data. To solve this issue, we first collect a novel benchmark on this setting with four diverse scenarios including concerts, sports games, gala shows, and contests, where each scenario contains 6 synchronized tracks recorded by different cameras. It contains ...",
    "pdf_url": "https://arxiv.org/pdf/2210.08737v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ]
  },
  {
    "id": "1912.05459v1",
    "title": "Deep Relevance Regularization: Interpretable and Robust Tumor Typing of Imaging Mass Spectrometry Data",
    "authors": [
      "Christian Etmann",
      "Maximilian Schmidt",
      "Jens Behrmann"
    ],
    "year": 2019,
    "abstract": "Neural networks have recently been established as a viable classification method for imaging mass spectrometry data for tumor typing. For multi-laboratory scenarios however, certain confounding factors may strongly impede their performance. In this work, we introduce Deep Relevance Regularization, a method of restricting what the neural network can focus on during classification, in order to improve the classification performance. We demonstrate how Deep Relevance Regularization robustifies neur...",
    "pdf_url": "https://arxiv.org/pdf/1912.05459v1",
    "categories": [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id": "1509.00070v1",
    "title": "In-Line-Test of Variability and Bit-Error-Rate of HfOx-Based Resistive Memory",
    "authors": [
      "B. L. Ji",
      "H. Li",
      "Q. Ye"
    ],
    "year": 2015,
    "abstract": "Spatial and temporal variability of HfOx-based resistive random access memory (RRAM) are investigated for manufacturing and product designs. Manufacturing variability is characterized at different levels including lots, wafers, and chips. Bit-error-rate (BER) is proposed as a holistic parameter for the write cycle resistance statistics. Using the electrical in-line-test cycle data, a method is developed to derive BERs as functions of the design margin, to provide guidance for technology evaluati...",
    "pdf_url": "https://arxiv.org/pdf/1509.00070v1",
    "categories": [
      "cs.ET",
      "cond-mat.mtrl-sci",
      "physics.data-an"
    ]
  },
  {
    "id": "2004.08116v1",
    "title": "Triplet Loss for Knowledge Distillation",
    "authors": [
      "Hideki Oki",
      "Motoshi Abe",
      "Junichi Miyao"
    ],
    "year": 2020,
    "abstract": "In recent years, deep learning has spread rapidly, and deeper, larger models have been proposed. However, the calculation cost becomes enormous as the size of the models becomes larger. Various techniques for compressing the size of the models have been proposed to improve performance while reducing computational costs. One of the methods to compress the size of the models is knowledge distillation (KD). Knowledge distillation is a technique for transferring knowledge of deep or ensemble models ...",
    "pdf_url": "https://arxiv.org/pdf/2004.08116v1",
    "categories": [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "id": "2405.09820v1",
    "title": "Densely Distilling Cumulative Knowledge for Continual Learning",
    "authors": [
      "Zenglin Shi",
      "Pei Liu",
      "Tong Su"
    ],
    "year": 2024,
    "abstract": "Continual learning, involving sequential training on diverse tasks, often faces catastrophic forgetting. While knowledge distillation-based approaches exhibit notable success in preventing forgetting, we pinpoint a limitation in their ability to distill the cumulative knowledge of all the previous tasks. To remedy this, we propose Dense Knowledge Distillation (DKD). DKD uses a task pool to track the model's capabilities. It partitions the output logits of the model into dense groups, each corres...",
    "pdf_url": "https://arxiv.org/pdf/2405.09820v1",
    "categories": [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "id": "1812.00660v1",
    "title": "Knowledge Distillation with Feature Maps for Image Classification",
    "authors": [
      "Wei-Chun Chen",
      "Chia-Che Chang",
      "Chien-Yu Lu"
    ],
    "year": 2018,
    "abstract": "The model reduction problem that eases the computation costs and latency of complex deep learning architectures has received an increasing number of investigations owing to its importance in model deployment. One promising method is knowledge distillation (KD), which creates a fast-to-execute student model to mimic a large teacher network. In this paper, we propose a method, called KDFM (Knowledge Distillation with Feature Maps), which improves the effectiveness of KD by learning the feature map...",
    "pdf_url": "https://arxiv.org/pdf/1812.00660v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "id": "1909.11723v3",
    "title": "Revisiting Knowledge Distillation via Label Smoothing Regularization",
    "authors": [
      "Li Yuan",
      "Francis E. H. Tay",
      "Guilin Li"
    ],
    "year": 2019,
    "abstract": "Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome teacher model into a lightweight student model. Its success is generally attributed to the privileged information on similarities among categories provided by the teacher model, and in this sense, only strong teacher models are deployed to teach weaker students in practice. In this work, we challenge this common belief by following experimental observations: 1) beyond the acknowledgment that the teacher can improve the stu...",
    "pdf_url": "https://arxiv.org/pdf/1909.11723v3",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id": "2306.10687v1",
    "title": "Categories of Response-Based, Feature-Based, and Relation-Based Knowledge Distillation",
    "authors": [
      "Chuanguang Yang",
      "Xinqiang Yu",
      "Zhulin An"
    ],
    "year": 2023,
    "abstract": "Deep neural networks have achieved remarkable performance for artificial intelligence tasks. The success behind intelligent systems often relies on large-scale models with high computational complexity and storage costs. The over-parameterized networks are often easy to optimize and can achieve better performance. However, it is challenging to deploy them over resource-limited edge-devices. Knowledge Distillation (KD) aims to optimize a lightweight network from the perspective of over-parameteri...",
    "pdf_url": "https://arxiv.org/pdf/2306.10687v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2007.06889v2",
    "title": "Knowledge Distillation for Multi-task Learning",
    "authors": [
      "Wei-Hong Li",
      "Hakan Bilen"
    ],
    "year": 2020,
    "abstract": "Multi-task learning (MTL) is to learn one single model that performs multiple tasks for achieving good performance on all tasks and lower cost on computation. Learning such a model requires to jointly optimize losses of a set of tasks with different difficulty levels, magnitudes, and characteristics (e.g. cross-entropy, Euclidean loss), leading to the imbalance problem in multi-task learning. To address the imbalance problem, we propose a knowledge distillation based method in this work. We firs...",
    "pdf_url": "https://arxiv.org/pdf/2007.06889v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2302.07215v1",
    "title": "Multi-teacher knowledge distillation as an effective method for compressing ensembles of neural networks",
    "authors": [
      "Konrad Zuchniak"
    ],
    "year": 2023,
    "abstract": "Deep learning has contributed greatly to many successes in artificial intelligence in recent years. Today, it is possible to train models that have thousands of layers and hundreds of billions of parameters. Large-scale deep models have achieved great success, but the enormous computational complexity and gigantic storage requirements make it extremely difficult to implement them in real-time applications. On the other hand, the size of the dataset is still a real problem in many domains. Data a...",
    "pdf_url": "https://arxiv.org/pdf/2302.07215v1",
    "categories": [
      "cs.LG"
    ]
  },
  {
    "id": "2008.00506v1",
    "title": "Differentiable Feature Aggregation Search for Knowledge Distillation",
    "authors": [
      "Yushuo Guan",
      "Pengyu Zhao",
      "Bingxuan Wang"
    ],
    "year": 2020,
    "abstract": "Knowledge distillation has become increasingly important in model compression. It boosts the performance of a miniaturized student network with the supervision of the output distribution and feature maps from a sophisticated teacher network. Some recent works introduce multi-teacher distillation to provide more supervision to the student network. However, the effectiveness of multi-teacher distillation methods are accompanied by costly computation resources. To tackle with both the efficiency an...",
    "pdf_url": "https://arxiv.org/pdf/2008.00506v1",
    "categories": [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "id": "2210.16611v2",
    "title": "Application of Knowledge Distillation to Multi-task Speech Representation Learning",
    "authors": [
      "Mine Kerpicci",
      "Van Nguyen",
      "Shuhua Zhang"
    ],
    "year": 2022,
    "abstract": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the a...",
    "pdf_url": "https://arxiv.org/pdf/2210.16611v2",
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ]
  },
  {
    "id": "2502.03034v1",
    "title": "Knowledge Distillation from Large Language Models for Household Energy Modeling",
    "authors": [
      "Mohannad Takrouri",
      "Nicol\u00e1s M. Cuadrado",
      "Martin Tak\u00e1\u010d"
    ],
    "year": 2025,
    "abstract": "Machine learning (ML) is increasingly vital for smart-grid research, yet restricted access to realistic, diverse data - often due to privacy concerns - slows progress and fuels doubts within the energy sector about adopting ML-based strategies. We propose integrating Large Language Models (LLMs) in energy modeling to generate realistic, culturally sensitive, and behavior-specific data for household energy usage across diverse geographies. In this study, we employ and compare five different LLMs ...",
    "pdf_url": "https://arxiv.org/pdf/2502.03034v1",
    "categories": [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id": "2303.11098v5",
    "title": "Understanding the Role of the Projector in Knowledge Distillation",
    "authors": [
      "Roy Miles",
      "Krystian Mikolajczyk"
    ],
    "year": 2023,
    "abstract": "In this paper we revisit the efficacy of knowledge distillation as a function matching and metric learning problem. In doing so we verify three important design decisions, namely the normalisation, soft maximum function, and projection layers as key ingredients. We theoretically show that the projector implicitly encodes information on past examples, enabling relational gradients for the student. We then show that the normalisation of representations is tightly coupled with the training dynamics...",
    "pdf_url": "https://arxiv.org/pdf/2303.11098v5",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "id": "2205.11246v1",
    "title": "[Re] Distilling Knowledge via Knowledge Review",
    "authors": [
      "Apoorva Verma",
      "Pranjal Gulati",
      "Sarthak Gupta"
    ],
    "year": 2022,
    "abstract": "This effort aims to reproduce the results of experiments and analyze the robustness of the review framework for knowledge distillation introduced in the CVPR '21 paper 'Distilling Knowledge via Knowledge Review' by Chen et al. Previous works in knowledge distillation only studied connections paths between the same levels of the student and the teacher, and cross-level connection paths had not been considered. Chen et al. propose a new residual learning framework to train a single student layer u...",
    "pdf_url": "https://arxiv.org/pdf/2205.11246v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id": "2306.09306v2",
    "title": "Propagating Knowledge Updates to LMs Through Distillation",
    "authors": [
      "Shankar Padmanabhan",
      "Yasumasa Onoe",
      "Michael J. Q. Zhang"
    ],
    "year": 2023,
    "abstract": "Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities and propagate that knowledge to enable broader i...",
    "pdf_url": "https://arxiv.org/pdf/2306.09306v2",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2302.14416v3",
    "title": "DREAM: Efficient Dataset Distillation by Representative Matching",
    "authors": [
      "Yanqing Liu",
      "Jianyang Gu",
      "Kai Wang"
    ],
    "year": 2023,
    "abstract": "Dataset distillation aims to synthesize small datasets with little information loss from original large-scale ones for reducing storage and training costs. Recent state-of-the-art methods mainly constrain the sample synthesis process by matching synthetic images and the original ones regarding gradients, embedding distributions, or training trajectories. Although there are various matching objectives, currently the strategy for selecting original images is limited to naive random sampling.\n  We ...",
    "pdf_url": "https://arxiv.org/pdf/2302.14416v3",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2511.04666v2",
    "title": "Forgetting is Everywhere",
    "authors": [
      "Ben Sanati",
      "Thomas L. Lee",
      "Trevor McInroe"
    ],
    "year": 2025,
    "abstract": "A fundamental challenge in developing general learning algorithms is their tendency to forget past knowledge when adapting to new data. Addressing this problem requires a principled understanding of forgetting; yet, despite decades of study, no unified definition has emerged that provides insights into the underlying dynamics of learning. We propose an algorithm- and task-agnostic theory that characterises forgetting as a lack of self-consistency in a learner's predictive distribution over futur...",
    "pdf_url": "https://arxiv.org/pdf/2511.04666v2",
    "categories": [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id": "2211.14177v1",
    "title": "Overcoming Catastrophic Forgetting by XAI",
    "authors": [
      "Giang Nguyen"
    ],
    "year": 2022,
    "abstract": "Explaining the behaviors of deep neural networks, usually considered as black boxes, is critical especially when they are now being adopted over diverse aspects of human life. Taking the advantages of interpretable machine learning (interpretable ML), this work proposes a novel tool called Catastrophic Forgetting Dissector (or CFD) to explain catastrophic forgetting in continual learning settings. We also introduce a new method called Critical Freezing based on the observations of our tool. Expe...",
    "pdf_url": "https://arxiv.org/pdf/2211.14177v1",
    "categories": [
      "cs.LG"
    ]
  },
  {
    "id": "2410.14159v2",
    "title": "Assessing Open-world Forgetting in Generative Image Model Customization",
    "authors": [
      "H\u00e9ctor Laria",
      "Alex Gomez-Villa",
      "Kai Wang"
    ],
    "year": 2024,
    "abstract": "Recent advances in diffusion models have significantly enhanced image generation capabilities. However, customizing these models with new classes often leads to unintended consequences that compromise their reliability. We introduce the concept of open-world forgetting to characterize the vast scope of these unintended alterations. Our work presents the first systematic investigation into open-world forgetting in diffusion models, focusing on semantic and appearance drift of representations. Usi...",
    "pdf_url": "https://arxiv.org/pdf/2410.14159v2",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ]
  },
  {
    "id": "2102.07686v4",
    "title": "Does the Adam Optimizer Exacerbate Catastrophic Forgetting?",
    "authors": [
      "Dylan R. Ashley",
      "Sina Ghiassian",
      "Richard S. Sutton"
    ],
    "year": 2021,
    "abstract": "Catastrophic forgetting remains a severe hindrance to the broad application of artificial neural networks (ANNs), however, it continues to be a poorly understood phenomenon. Despite the extensive amount of work on catastrophic forgetting, we argue that it is still unclear how exactly the phenomenon should be quantified, and, moreover, to what degree all of the choices we make when designing learning systems affect the amount of catastrophic forgetting. We use various testbeds from the reinforcem...",
    "pdf_url": "https://arxiv.org/pdf/2102.07686v4",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ]
  },
  {
    "id": "2512.13706v1",
    "title": "Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through Mixed Training",
    "authors": [
      "John Graham Reynolds"
    ],
    "year": 2025,
    "abstract": "When finetuning large language models for specialized tasks such as mathematical reasoning, models exhibit catastrophic forgetting, losing previously learned capabilities. We investigate this by finetuning Flan-T5-Base (250M parameters) on the DeepMind Mathematics dataset and measuring forgetting on MultiNLI. Math-only training improves mathematical accuracy from 3.1\\% to 12.0\\% but causes NLI accuracy to collapse from 81.0\\% to 16.5\\%--a 64.5 percentage point drop occurring within the first 1,0...",
    "pdf_url": "https://arxiv.org/pdf/2512.13706v1",
    "categories": [
      "cs.LG",
      "cs.CL"
    ]
  },
  {
    "id": "2305.12740v1",
    "title": "Can We Edit Factual Knowledge by In-Context Learning?",
    "authors": [
      "Ce Zheng",
      "Lei Li",
      "Qingxiu Dong"
    ],
    "year": 2023,
    "abstract": "Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or out-dated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs. Inspire...",
    "pdf_url": "https://arxiv.org/pdf/2305.12740v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2510.07896v1",
    "title": "ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall",
    "authors": [
      "Jiayu Yang",
      "Yuxuan Fan",
      "Songning Lai"
    ],
    "year": 2025,
    "abstract": "Large Language Models (LLMs) require efficient knowledge editing (KE) to update factual information, yet existing methods exhibit significant performance decay in multi-hop factual recall. This failure is particularly acute when edits involve intermediate implicit subjects within reasoning chains. Through causal analysis, we reveal that this limitation stems from an oversight of how chained knowledge is dynamically represented and utilized at the neuron level. We discover that during multi hop r...",
    "pdf_url": "https://arxiv.org/pdf/2510.07896v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2402.18099v3",
    "title": "Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models",
    "authors": [
      "Derong Xu",
      "Ziheng Zhang",
      "Zhihong Zhu"
    ],
    "year": 2024,
    "abstract": "Model editing aims to precisely alter the behaviors of large language models (LLMs) in relation to specific knowledge, while leaving unrelated knowledge intact. This approach has proven effective in addressing issues of hallucination and outdated information in LLMs. However, the potential of using model editing to modify knowledge in the medical field remains largely unexplored, even though resolving hallucination is a pressing need in this area. Our observations indicate that current methods f...",
    "pdf_url": "https://arxiv.org/pdf/2402.18099v3",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2402.11900v2",
    "title": "Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models",
    "authors": [
      "Tianjie Ju",
      "Yijin Chen",
      "Xinwei Yuan"
    ],
    "year": 2024,
    "abstract": "Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for LLMs to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through K...",
    "pdf_url": "https://arxiv.org/pdf/2402.11900v2",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2104.08164v2",
    "title": "Editing Factual Knowledge in Language Models",
    "authors": [
      "Nicola De Cao",
      "Wilker Aziz",
      "Ivan Titov"
    ],
    "year": 2021,
    "abstract": "The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix 'bugs' or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient, Knowledge...",
    "pdf_url": "https://arxiv.org/pdf/2104.08164v2",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id": "2402.13093v2",
    "title": "Event-level Knowledge Editing",
    "authors": [
      "Hao Peng",
      "Xiaozhi Wang",
      "Chunyang Li"
    ],
    "year": 2024,
    "abstract": "Knowledge editing aims at updating knowledge of large language models (LLMs) to prevent them from becoming outdated. Existing work edits LLMs at the level of factual knowledge triplets. However, natural knowledge updates in the real world come from the occurrences of new events rather than direct changes in factual triplets. In this paper, we propose a new task setting: event-level knowledge editing, which directly edits new events into LLMs and improves over conventional triplet-level editing o...",
    "pdf_url": "https://arxiv.org/pdf/2402.13093v2",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2503.01090v2",
    "title": "Precise Localization of Memories: A Fine-grained Neuron-level Knowledge Editing Technique for LLMs",
    "authors": [
      "Haowen Pan",
      "Xiaozhi Wang",
      "Yixin Cao"
    ],
    "year": 2025,
    "abstract": "Knowledge editing aims to update outdated information in Large Language Models (LLMs). A representative line of study is locate-then-edit methods, which typically employ causal tracing to identify the modules responsible for recalling factual knowledge about entities. However, we find these methods are often sensitive only to changes in the subject entity, leaving them less effective at adapting to changes in relations. This limitation results in poor editing locality, which can lead to the pers...",
    "pdf_url": "https://arxiv.org/pdf/2503.01090v2",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2206.04801v1",
    "title": "Learning Attention-based Representations from Multiple Patterns for Relation Prediction in Knowledge Graphs",
    "authors": [
      "V\u00edtor Louren\u00e7o",
      "Aline Paes"
    ],
    "year": 2022,
    "abstract": "Knowledge bases, and their representations in the form of knowledge graphs (KGs), are naturally incomplete. Since scientific and industrial applications have extensively adopted them, there is a high demand for solutions that complete their information. Several recent works tackle this challenge by learning embeddings for entities and relations, then employing them to predict new relations among the entities. Despite their aggrandizement, most of those methods focus only on the local neighbors o...",
    "pdf_url": "https://arxiv.org/pdf/2206.04801v1",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id": "2412.20637v1",
    "title": "Knowledge Editing for Large Language Model with Knowledge Neuronal Ensemble",
    "authors": [
      "Yongchang Li",
      "Yujin Zhu",
      "Tao Yan"
    ],
    "year": 2024,
    "abstract": "As real-world knowledge is constantly evolving, ensuring the timeliness and accuracy of a model's knowledge is crucial. This has made knowledge editing in large language models increasingly important. However, existing knowledge editing methods face several challenges, including parameter localization coupling, imprecise localization, and a lack of dynamic interaction across layers. In this paper, we propose a novel knowledge editing method called Knowledge Neuronal Ensemble (KNE). A knowledge n...",
    "pdf_url": "https://arxiv.org/pdf/2412.20637v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2511.11017v1",
    "title": "AI Agent-Driven Framework for Automated Product Knowledge Graph Construction in E-Commerce",
    "authors": [
      "Dimitar Peshevski",
      "Riste Stojanov",
      "Dimitar Trajanov"
    ],
    "year": 2025,
    "abstract": "The rapid expansion of e-commerce platforms generates vast amounts of unstructured product data, creating significant challenges for information retrieval, recommendation systems, and data analytics. Knowledge Graphs (KGs) offer a structured, interpretable format to organize such data, yet constructing product-specific KGs remains a complex and manual process. This paper introduces a fully automated, AI agent-driven framework for constructing product knowledge graphs directly from unstructured p...",
    "pdf_url": "https://arxiv.org/pdf/2511.11017v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "id": "2504.20900v1",
    "title": "Evaluating Generative Models for Tabular Data: Novel Metrics and Benchmarking",
    "authors": [
      "Dayananda Herurkar",
      "Ahmad Ali",
      "Andreas Dengel"
    ],
    "year": 2025,
    "abstract": "Generative models have revolutionized multiple domains, yet their application to tabular data remains underexplored. Evaluating generative models for tabular data presents unique challenges due to structural complexity, large-scale variability, and mixed data types, making it difficult to intuitively capture intricate patterns. Existing evaluation metrics offer only partial insights, lacking a comprehensive measure of generative performance. To address this limitation, we propose three novel eva...",
    "pdf_url": "https://arxiv.org/pdf/2504.20900v1",
    "categories": [
      "cs.LG"
    ]
  },
  {
    "id": "1909.07917v2",
    "title": "Toward Efficient Evaluation of Logic Encryption Schemes: Models and Metrics",
    "authors": [
      "Yinghua Hu",
      "Vivek V. Menon",
      "Andrew Schmidt"
    ],
    "year": 2019,
    "abstract": "Research in logic encryption over the last decade has resulted in various techniques to prevent different security threats such as Trojan insertion, intellectual property leakage, and reverse engineering. However, there is little agreement on a uniform set of metrics and models to efficiently assess the achieved security level and the trade-offs between security and overhead. This paper addresses the above challenges by relying on a general logic encryption model that can encompass all the exist...",
    "pdf_url": "https://arxiv.org/pdf/1909.07917v2",
    "categories": [
      "cs.CR",
      "eess.SY"
    ]
  },
  {
    "id": "2009.01557v1",
    "title": "Evaluation of Software Product Quality Metrics",
    "authors": [
      "Arthur-Jozsef Molnar",
      "Alexandra Neam\u0163u",
      "Simona Motogna"
    ],
    "year": 2020,
    "abstract": "Computing devices and associated software govern everyday life, and form the backbone of safety critical systems in banking, healthcare, automotive and other fields. Increasing system complexity, quickly evolving technologies and paradigm shifts have kept software quality research at the forefront. Standards such as ISO's 25010 express it in terms of sub-characteristics such as maintainability, reliability and security. A significant body of literature attempts to link these subcharacteristics w...",
    "pdf_url": "https://arxiv.org/pdf/2009.01557v1",
    "categories": [
      "cs.SE"
    ]
  },
  {
    "id": "2408.12398v1",
    "title": "A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation",
    "authors": [
      "Weijia Zhang",
      "Mohammad Aliannejadi",
      "Jiahuan Pei"
    ],
    "year": 2024,
    "abstract": "Large language models (LLMs) often generate content with unsupported or unverifiable content, known as \"hallucinations.\" To address this, retrieval-augmented LLMs are employed to include citations in their content, grounding the content in verifiable sources. Despite such developments, manually assessing how well a citation supports the associated statement remains a major challenge. Previous studies tackle this challenge by leveraging faithfulness metrics to estimate citation support automatica...",
    "pdf_url": "https://arxiv.org/pdf/2408.12398v1",
    "categories": [
      "cs.IR",
      "cs.CL"
    ]
  },
  {
    "id": "2505.15702v2",
    "title": "LyapLock: Bounded Knowledge Preservation in Sequential Large Language Model Editing",
    "authors": [
      "Peng Wang",
      "Biyu Zhou",
      "Xuehai Tang"
    ],
    "year": 2025,
    "abstract": "Large Language Models often contain factually incorrect or outdated knowledge, giving rise to model editing methods for precise knowledge updates. However, current mainstream locate-then-edit approaches exhibit a progressive performance decline during sequential editing, due to inadequate mechanisms for long-term knowledge preservation. To tackle this, we model the sequential editing as a constrained stochastic programming. Given the challenges posed by the cumulative preservation error constrai...",
    "pdf_url": "https://arxiv.org/pdf/2505.15702v2",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2402.13048v1",
    "title": "Stable Knowledge Editing in Large Language Models",
    "authors": [
      "Zihao Wei",
      "Liang Pang",
      "Hanxing Ding"
    ],
    "year": 2024,
    "abstract": "Efficient knowledge editing of large language models is crucial for replacing obsolete information or incorporating specialized knowledge on a large scale. However, previous methods implicitly assume that knowledge is localized and isolated within the model, an assumption that oversimplifies the interconnected nature of model knowledge. The premise of localization results in an incomplete knowledge editing, whereas an isolated assumption may impair both other knowledge and general abilities. It ...",
    "pdf_url": "https://arxiv.org/pdf/2402.13048v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2404.04990v3",
    "title": "MLaKE: Multilingual Knowledge Editing Benchmark for Large Language Models",
    "authors": [
      "Zihao Wei",
      "Jingcheng Deng",
      "Liang Pang"
    ],
    "year": 2024,
    "abstract": "The extensive utilization of large language models (LLMs) underscores the crucial necessity for precise and contemporary knowledge embedded within their intrinsic parameters. Existing research on knowledge editing primarily concentrates on monolingual scenarios, neglecting the complexities presented by multilingual contexts and multi-hop reasoning. To address these challenges, our study introduces MLaKE (Multilingual Language Knowledge Editing), a novel benchmark comprising 4072 multi-hop and 53...",
    "pdf_url": "https://arxiv.org/pdf/2404.04990v3",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2505.23026v2",
    "title": "Context-Robust Knowledge Editing for Language Models",
    "authors": [
      "Haewon Park",
      "Gyubin Choi",
      "Minjun Kim"
    ],
    "year": 2025,
    "abstract": "Knowledge editing (KE) methods offer an efficient way to modify knowledge in large language models. Current KE evaluations typically assess editing success by considering only the edited knowledge without any preceding contexts. In real-world applications, however, preceding contexts often trigger the retrieval of the original knowledge and undermine the intended edit. To address this issue, we develop CHED -- a benchmark designed to evaluate the context robustness of KE methods. Evaluations on ...",
    "pdf_url": "https://arxiv.org/pdf/2505.23026v2",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2307.05639v2",
    "title": "Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks",
    "authors": [
      "Danny D'Agostino",
      "Ilija Ilievski",
      "Christine Annette Shoemaker"
    ],
    "year": 2023,
    "abstract": "Providing a model that achieves a strong predictive performance and is simultaneously interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives. To address this challenge, we propose a modification of the radial basis function neural network model by equipping its Gaussian kernel with a learnable precision matrix. We show that precious information is contained in the spectrum of the precision matrix that can...",
    "pdf_url": "https://arxiv.org/pdf/2307.05639v2",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "stat.ML"
    ]
  },
  {
    "id": "1905.05918v1",
    "title": "A Neural Network-Evolutionary Computational Framework for Remaining Useful Life Estimation of Mechanical Systems",
    "authors": [
      "David Laredo",
      "Zhaoyin Chen",
      "Oliver Sch\u00fctze"
    ],
    "year": 2019,
    "abstract": "This paper presents a framework for estimating the remaining useful life (RUL) of mechanical systems. The framework consists of a multi-layer perceptron and an evolutionary algorithm for optimizing the data-related parameters. The framework makes use of a strided time window to estimate the RUL for mechanical components. Tuning the data-related parameters can become a very time consuming task. The framework presented here automatically reshapes the data such that the efficiency of the model is i...",
    "pdf_url": "https://arxiv.org/pdf/1905.05918v1",
    "categories": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ]
  },
  {
    "id": "2103.07492v4",
    "title": "Continual Learning for Recurrent Neural Networks: an Empirical Evaluation",
    "authors": [
      "Andrea Cossu",
      "Antonio Carta",
      "Vincenzo Lomonaco"
    ],
    "year": 2021,
    "abstract": "Learning continuously during all model lifetime is fundamental to deploy machine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like natural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-specific and whose assessment is based on he...",
    "pdf_url": "https://arxiv.org/pdf/2103.07492v4",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id": "1512.00242v1",
    "title": "Towards Dropout Training for Convolutional Neural Networks",
    "authors": [
      "Haibing Wu",
      "Xiaodong Gu"
    ],
    "year": 2015,
    "abstract": "Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in convolutional and pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used m...",
    "pdf_url": "https://arxiv.org/pdf/1512.00242v1",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.NE"
    ]
  },
  {
    "id": "2205.13273v1",
    "title": "Acute Lymphoblastic Leukemia Detection Using Hypercomplex-Valued Convolutional Neural Networks",
    "authors": [
      "Guilherme Vieira",
      "Marcos Eduardo Valle"
    ],
    "year": 2022,
    "abstract": "This paper features convolutional neural networks defined on hypercomplex algebras applied to classify lymphocytes in blood smear digital microscopic images. Such classification is helpful for the diagnosis of acute lymphoblast leukemia (ALL), a type of blood cancer. We perform the classification task using eight hypercomplex-valued convolutional neural networks (HvCNNs) along with real-valued convolutional networks. Our results show that HvCNNs perform better than the real-valued model, showcas...",
    "pdf_url": "https://arxiv.org/pdf/2205.13273v1",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.NE",
      "eess.IV"
    ]
  },
  {
    "id": "1003.3081v1",
    "title": "Optimal hierarchical modular topologies for producing limited sustained activation of neural networks",
    "authors": [
      "Marcus Kaiser",
      "Claus C. Hilgetag"
    ],
    "year": 2010,
    "abstract": "An essential requirement for the representation of functional patterns in complex neural networks, such as the mammalian cerebral cortex, is the existence of stable regimes of network activation, typically arising from a limited parameter range. In this range of limited sustained activity (LSA), the activity of neural populations in the network persists between the extremes of either quickly dying out or activating the whole network. Hierarchical modular networks were previously found to show a ...",
    "pdf_url": "https://arxiv.org/pdf/1003.3081v1",
    "categories": [
      "q-bio.NC",
      "cond-mat.dis-nn",
      "physics.soc-ph"
    ]
  },
  {
    "id": "2001.10696v5",
    "title": "Spiking Inception Module for Multi-layer Unsupervised Spiking Neural Networks",
    "authors": [
      "Mingyuan Meng",
      "Xingyu Yang",
      "Shanlin Xiao"
    ],
    "year": 2020,
    "abstract": "Spiking Neural Network (SNN), as a brain-inspired approach, is attracting attention due to its potential to produce ultra-high-energy-efficient hardware. Competitive learning based on Spike-Timing-Dependent Plasticity (STDP) is a popular method to train an unsupervised SNN. However, previous unsupervised SNNs trained through this method are limited to a shallow network with only one learnable layer and cannot achieve satisfactory results when compared with multi-layer SNNs. In this paper, we eas...",
    "pdf_url": "https://arxiv.org/pdf/2001.10696v5",
    "categories": [
      "cs.NE",
      "cs.LG",
      "q-bio.NC"
    ]
  },
  {
    "id": "1812.10761v3",
    "title": "Improving Generalization of Deep Neural Networks by Leveraging Margin Distribution",
    "authors": [
      "Shen-Huan Lyu",
      "Lu Wang",
      "Zhi-Hua Zhou"
    ],
    "year": 2018,
    "abstract": "Recent research has used margin theory to analyze the generalization performance for deep neural networks (DNNs). The existed results are almost based on the spectrally-normalized minimum margin. However, optimizing the minimum margin ignores a mass of information about the entire margin distribution, which is crucial to generalization performance. In this paper, we prove a generalization upper bound dominated by the statistics of the entire margin distribution. Compared with the minimum margin ...",
    "pdf_url": "https://arxiv.org/pdf/1812.10761v3",
    "categories": [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id": "1911.06009v1",
    "title": "A Recurrent Probabilistic Neural Network with Dimensionality Reduction Based on Time-series Discriminant Component Analysis",
    "authors": [
      "Hideaki Hayashi",
      "Taro Shibanoki",
      "Keisuke Shima"
    ],
    "year": 2019,
    "abstract": "This paper proposes a probabilistic neural network developed on the basis of time-series discriminant component analysis (TSDCA) that can be used to classify high-dimensional time-series patterns. TSDCA involves the compression of high-dimensional time series into a lower-dimensional space using a set of orthogonal transformations and the calculation of posterior probabilities based on a continuous-density hidden Markov model with a Gaussian mixture model expressed in the reduced-dimensional spa...",
    "pdf_url": "https://arxiv.org/pdf/1911.06009v1",
    "categories": [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id": "2211.12792v2",
    "title": "MECCH: Metapath Context Convolution-based Heterogeneous Graph Neural Networks",
    "authors": [
      "Xinyu Fu",
      "Irwin King"
    ],
    "year": 2022,
    "abstract": "Heterogeneous graph neural networks (HGNNs) were proposed for representation learning on structural data with multiple types of nodes and edges. To deal with the performance degradation issue when HGNNs become deep, researchers combine metapaths into HGNNs to associate nodes closely related in semantics but far apart in the graph. However, existing metapath-based models suffer from either information loss or high computation costs. To address these problems, we present a novel Metapath Context C...",
    "pdf_url": "https://arxiv.org/pdf/2211.12792v2",
    "categories": [
      "cs.LG"
    ]
  },
  {
    "id": "2102.03983v1",
    "title": "Partial Is Better Than All: Revisiting Fine-tuning Strategy for Few-shot Learning",
    "authors": [
      "Zhiqiang Shen",
      "Zechun Liu",
      "Jie Qin"
    ],
    "year": 2021,
    "abstract": "The goal of few-shot learning is to learn a classifier that can recognize unseen classes from limited support data with labels. A common practice for this task is to train a model on the base set first and then transfer to novel classes through fine-tuning (Here fine-tuning procedure is defined as transferring knowledge from base to novel data, i.e. learning to transfer in few-shot scenario.) or meta-learning. However, as the base classes have no overlap to the novel set, simply transferring who...",
    "pdf_url": "https://arxiv.org/pdf/2102.03983v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id": "2403.00946v3",
    "title": "Fine-tuning with Very Large Dropout",
    "authors": [
      "Jianyu Zhang",
      "L\u00e9on Bottou"
    ],
    "year": 2024,
    "abstract": "It is impossible today to pretend that the practice of machine learning is always compatible with the idea that training and testing data follow the same distribution. Several authors have recently used ensemble techniques to show how scenarios involving multiple data distributions are best served by representations that are both richer than those obtained by regularizing for the best in-distribution performance, and richer than those obtained under the influence of the implicit sparsity bias of...",
    "pdf_url": "https://arxiv.org/pdf/2403.00946v3",
    "categories": [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "id": "2110.06500v2",
    "title": "Differentially Private Fine-tuning of Language Models",
    "authors": [
      "Da Yu",
      "Saurabh Naik",
      "Arturs Backurs"
    ],
    "year": 2021,
    "abstract": "We give simpler, sparser, and faster algorithms for differentially private fine-tuning of large-scale pre-trained language models, which achieve the state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks. We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning. Our experiments show that differentially private adaptations of these approaches outperform previous private algorithms in three important di...",
    "pdf_url": "https://arxiv.org/pdf/2110.06500v2",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CR",
      "stat.ML"
    ]
  },
  {
    "id": "2110.05457v1",
    "title": "Legged Robots that Keep on Learning: Fine-Tuning Locomotion Policies in the Real World",
    "authors": [
      "Laura Smith",
      "J. Chase Kew",
      "Xue Bin Peng"
    ],
    "year": 2021,
    "abstract": "Legged robots are physically capable of traversing a wide range of challenging environments, but designing controllers that are sufficiently robust to handle this diversity has been a long-standing challenge in robotics. Reinforcement learning presents an appealing approach for automating the controller design process and has been able to produce remarkably robust controllers when trained in a suitable range of environments. However, it is difficult to predict all likely conditions the robot wil...",
    "pdf_url": "https://arxiv.org/pdf/2110.05457v1",
    "categories": [
      "cs.RO"
    ]
  },
  {
    "id": "2408.07888v2",
    "title": "Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies in Medical Question Answering",
    "authors": [
      "Yushi Yang",
      "Andrew M. Bean",
      "Robert McCraith"
    ],
    "year": 2024,
    "abstract": "Fine-tuning Large Language Models (LLMs) incurs considerable training costs, driving the need for data-efficient training with optimised data ordering. Human-inspired strategies offer a solution by organising data based on human learning practices. This study evaluates the fine-tuning efficiency of five human-inspired strategies across four language models, three datasets, and both human- and LLM-labelled data in the context of medical question answering. These strategies achieve the best accura...",
    "pdf_url": "https://arxiv.org/pdf/2408.07888v2",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2105.08021v2",
    "title": "Stage-wise Fine-tuning for Graph-to-Text Generation",
    "authors": [
      "Qingyun Wang",
      "Semih Yavuz",
      "Victoria Lin"
    ],
    "year": 2021,
    "abstract": "Graph-to-text generation has benefited from pre-trained language models (PLMs) in achieving better performance than structured graph encoders. However, they fail to fully utilize the structure information of the input graph. In this paper, we aim to further improve the performance of the pre-trained language model by proposing a structured graph-to-text model with a two-step fine-tuning mechanism which first fine-tunes the model on Wikipedia before adapting to the graph-to-text generation. In ad...",
    "pdf_url": "https://arxiv.org/pdf/2105.08021v2",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "1801.10059v1",
    "title": "Is The Universal Matter - Antimatter Asymmetry Fine Tuned?",
    "authors": [
      "Gary Steigman",
      "Robert J. Scherrer"
    ],
    "year": 2018,
    "abstract": "The asymmetry between matter and antimatter is key to the existence and nature of our Universe. A measure of the matter - antimatter asymmetry of the Universe is provided by the present value of the universal ratio of baryons (baryons minus antibaryons) to photons (or the ratio of baryons to entropy). The baryon asymmetry parameter is an important physical and cosmological parameter. But how fine tuned is it? A \"natural\" value for this parameter is zero, corresponding to equal amounts of matter ...",
    "pdf_url": "https://arxiv.org/pdf/1801.10059v1",
    "categories": [
      "astro-ph.CO",
      "hep-ph"
    ]
  },
  {
    "id": "2210.12607v1",
    "title": "Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models",
    "authors": [
      "Victor S. Bursztyn",
      "David Demeter",
      "Doug Downey"
    ],
    "year": 2022,
    "abstract": "How to usefully encode compositional task structure has long been a core challenge in AI. Recent work in chain of thought prompting has shown that for very large neural language models (LMs), explicitly demonstrating the inferential steps involved in a target task may improve performance over end-to-end learning that focuses on the target task alone. However, chain of thought prompting has significant limitations due to its dependency on huge pretrained LMs. In this work, we present compositiona...",
    "pdf_url": "https://arxiv.org/pdf/2210.12607v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id": "2408.03099v1",
    "title": "Topic Modeling with Fine-tuning LLMs and Bag of Sentences",
    "authors": [
      "Johannes Schneider"
    ],
    "year": 2024,
    "abstract": "Large language models (LLM)'s are increasingly used for topic modeling outperforming classical topic models such as LDA. Commonly, pre-trained LLM encoders such as BERT are used out-of-the-box despite the fact that fine-tuning is known to improve LLMs considerably. The challenge lies in obtaining a suitable (labeled) dataset for fine-tuning. In this paper, we use the recent idea to use bag of sentences as the elementary unit in computing topics. In turn, we derive an approach FT-Topic to perform...",
    "pdf_url": "https://arxiv.org/pdf/2408.03099v1",
    "categories": [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id": "2004.10190v2",
    "title": "Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic Reinforcement Learning",
    "authors": [
      "Ryan Julian",
      "Benjamin Swanson",
      "Gaurav S. Sukhatme"
    ],
    "year": 2020,
    "abstract": "One of the great promises of robot learning systems is that they will be able to learn from their mistakes and continuously adapt to ever-changing environments. Despite this potential, most of the robot learning systems today are deployed as a fixed policy and they are not being adapted after their deployment. Can we efficiently adapt previously learned behaviors to new environments, objects and percepts in the real world? In this paper, we present a method and empirical evidence towards a robot...",
    "pdf_url": "https://arxiv.org/pdf/2004.10190v2",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.RO",
      "stat.ML"
    ]
  },
  {
    "id": "1212.2655v2",
    "title": "Radiative natural supersymmetry: Reconciling electroweak fine-tuning and the Higgs boson mass",
    "authors": [
      "Howard Baer",
      "Vernon Barger",
      "Peisi Huang"
    ],
    "year": 2012,
    "abstract": "Models of natural supersymmetry seek to solve the little hierarchy problem by positing a spectrum of light higgsinos <~ 200-300 GeV and light top squarks <~ 600 GeV along with very heavy squarks and TeV-scale gluinos. Such models have low electroweak fine-tuning and satisfy the LHC constraints. However, in the context of the MSSM, they predict too low a value of m(h), are frequently in conflict with the measured b\\to s\u03b3branching fraction and the relic density of thermally produced higgsino-like ...",
    "pdf_url": "https://arxiv.org/pdf/1212.2655v2",
    "categories": [
      "hep-ph"
    ]
  },
  {
    "id": "2501.19389v3",
    "title": "Federated Sketching LoRA: A Flexible Framework for Heterogeneous Collaborative Fine-Tuning of LLMs",
    "authors": [
      "Wenzhi Fang",
      "Dong-Jun Han",
      "Liangqi Yuan"
    ],
    "year": 2025,
    "abstract": "Fine-tuning large language models (LLMs) on resource-constrained clients remains a challenging problem. Recent works have fused low-rank adaptation (LoRA) techniques with federated fine-tuning to mitigate challenges associated with client model sizes and data scarcity. Still, the heterogeneity of resources remains a critical bottleneck: while higher-rank modules generally enhance performance, varying client capabilities constrain LoRA's feasible rank range. Existing approaches attempting to reso...",
    "pdf_url": "https://arxiv.org/pdf/2501.19389v3",
    "categories": [
      "cs.LG"
    ]
  },
  {
    "id": "2310.03059v8",
    "title": "Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models",
    "authors": [
      "Yiwen Tang",
      "Ray Zhang",
      "Zoey Guo"
    ],
    "year": 2023,
    "abstract": "The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained model...",
    "pdf_url": "https://arxiv.org/pdf/2310.03059v8",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id": "2505.23724v3",
    "title": "SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA",
    "authors": [
      "Minrui Luo",
      "Fuhang Kuang",
      "Yu Wang"
    ],
    "year": 2025,
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), are indispensable for efficiently customizing Large Language Models (LLMs). However, vanilla LoRA suffers from slow convergence speed and knowledge forgetting problems. Recent studies have leveraged the power of designed LoRA initialization, to enhance the fine-tuning efficiency, or to preserve knowledge in the pre-trained LLM. However, none of these works can address the two cases at the same time. To this ...",
    "pdf_url": "https://arxiv.org/pdf/2505.23724v3",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  }
]